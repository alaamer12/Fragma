{"cells":[{"cell_type":"markdown","metadata":{"id":"VtOchqjyNsDD"},"source":["# 🧪 Fragma: Overview & Navigation\n","*Your guide to understanding and navigating this fragment detection project.*\n","\n","---\n","\n","\n","## 🧭 Table of Contents\n","\n","- 📘 [Project Introduction](#project-introduction)\n","- 🗂️ [Project Structure](#project-structure)\n","- 📓 [Notebook Structure](#notebook-structure)\n","- 🧰 [Dependencies & Setup](#dependencies--setup)\n","- 🚀 [Getting Started](#getting-started)\n","- 📊 [Project Status](#project-status)\n","- 📚 [Resources](#resources)\n","- 👥 [Contributors](#contributors)\n","- 📝 [Documentation](#documentation)\n","\n","\n","## 🧠 Project Introduction\n","\n","**🔍 Overview:**  \n","Fragma is a specialized model designed to detect sentence fragments for optimizing autocomplete systems. By identifying and classifying text fragments, Fragma helps autocomplete models provide more contextually relevant suggestions.\n","\n","**🎯 Goals:**  \n","- Create a robust fragment detection model for autocomplete optimization\n","- Build a comprehensive dataset of labeled sentence fragments\n","- Develop intelligent text preprocessing and feature extraction pipelines\n","- Enable real-time fragment detection in user input\n","\n","**📘 Context:**  \n","Modern autocomplete systems often struggle with partial or incomplete sentences. Fragma addresses this by learning to identify and classify text fragments, enabling more accurate and context-aware suggestions.\n","\n","**🗓 Metadata:**  \n","- **Created:** 2025-05-12  \n","- **Last Updated:** 2025-05-12  \n","- **Status:** Model Development Phase\n","- **Version:** 1.0.0  "]},{"cell_type":"markdown","metadata":{"id":"wht9-JGVNsDF"},"source":["## 🗂️ Project Structure\n","\n","### Core Components:\n","\n","1. **Fragment Detector Dataset Creator** (`fd_dataset_creator_script.py`)\n","   - Processes raw conversational data\n","   - Applies intelligent splitting rules\n","   - Balances dataset for model training\n","\n","2. **Text Preprocessing Pipeline** (`preprocessor.py`)\n","   - Unicode normalization and cleaning\n","   - HTML entity and whitespace handling\n","   - Emoji/emoticon removal\n","   - Advanced tokenization\n","\n","3. **Linguistic Features** (`fd_linguistic_features.py`)\n","   - Word lists and patterns\n","   - Feature descriptions\n","   - Regex patterns for analysis\n","\n","4. **Dataset Expander** (`fd_ds_expander.py`)\n","   - Feature extraction\n","   - Dataset augmentation\n","   - Linguistic analysis"]},{"cell_type":"markdown","metadata":{"id":"MIHysK6UNsDF"},"source":["## 📓 Notebook Structure\n","\n","Below is the execution order and description of each notebook in this project:\n","\n","| 🔢 Order | 📓 Notebook | 📝 Description |\n","|----------|------------|----------------|\n","| 0 | [00-Fragma-Overview.ipynb](https://colab.research.google.com/drive/1Mrnk4I4nD-aty1lEdS08wyGA1ywgRHV5?usp=sharing) | Project overview and setup |\n","| 1 | [01-Data-Loading.ipynb](https://colab.research.google.com/drive/1NZeZYBdgr6QsVLg8je0F2D0DHuYuuxsd?usp=drive_link) | Dataset loading and initial exploration |\n","| 2 | [02-Fragment-Detection.ipynb](https://colab.research.google.com/drive/1PL9wJr-zn8dFTuU5y8HuMzY4fTedZKtL?usp=sharing) | Fragment detection and preprocessing |\n","| 3 | [03-Random-Forest-Regressor.ipynb](https://colab.research.google.com/drive/196Spb8P56B8fwFdnkp8Kmiq9uitzLrH1?usp=sharing) | Random Forest model training and evaluation |\n","| 4 | [04-LSTM-Model.ipynb](https://colab.research.google.com/drive/1Fg7Tw1Xj3O9NuwCdn_oztEG6Q4r7RWo4?usp=sharing) | Deep learning model using LSTM architecture |\n","| 5 | [05-GUI.ipynb](https://colab.research.google.com/drive/1eoX1dSyDxFyK7GxWFUrwyxls0PIbFNdk?usp=sharing) | Graphical interface to test both ML and DL models |\n","\n","> 🌐 **Note:** All notebooks are accessible via Google Colab links above.\n"]},{"cell_type":"markdown","metadata":{"id":"1xr8tNrKNsDG"},"source":["## 📦 Dependencies & Setup\n","\n","**💡 Required Packages:**\n","```python\n","pandas         # Data manipulation\n","tqdm          # Progress bars\n","nltk          # NLP tools\n","ftfy          # Unicode fixing\n","emoji         # Emoji handling\n","textblob      # Text processing\n","contractions  # Contraction expansion\n","colab_print   # Pretty Printing\n","```\n","\n","**🔧 Installation:**\n","```bash\n","pip install pandas tqdm nltk ftfy emoji textblob contractions\n","```\n","\n","**📎 NLTK Data:**\n","```python\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","```\n"]},{"cell_type":"markdown","source":["### 🔧 Custom Utilities\n","\n","This project uses the custom `colab_print` utility for consistent output formatting:\n","\n","**Installation:**\n","```python\n","!pip install colab-print\n","```\n","\n","**Key Functions:**\n","\n","| Function     | Purpose                                          |\n","|--------------|--------------------------------------------------|\n","| `header()`   | Print section headers with consistent formatting |\n","| `title()`    | Highlight subsection titles                      |\n","| `table()`    | Pretty-print tabular data                        |\n","| `info()`     | Display informational messages                   |\n","| `success()`  | Show success messages                            |\n","| `warning()`  | Display warning messages                         |\n","| `error()`    | Show error messages                              |\n","\n","\n","> 📘 **Note:** All notebooks use these formatting functions for consistent output."],"metadata":{"id":"jjE1Ge3WOe9U"}},{"cell_type":"markdown","source":["```bash\n","# Install required packages\n","!pip install pandas tqdm nltk ftfy emoji textblob contractions\n","```\n","\n","```python\n","# Download NLTK data\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","```"],"metadata":{"id":"h4obq8scOCH0"}},{"cell_type":"markdown","metadata":{"id":"d11p7FwUNsDH"},"source":["## 🚀 Getting Started\n","\n","### Dataset Creation\n","\n","```python\n","from fd_dataset_creator_script import process_dataset\n","\n","# Process and create dataset\n","process_dataset(\n","    input_file='raw_data.csv',\n","    output_file='processed_data.csv',\n","    balance_strategy='expand'  # or 'reduce'\n",")\n","```\n","\n","### Text Preprocessing\n","\n","```python\n","from preprocessor import preprocess_df\n","import pandas as pd\n","\n","# Load your data\n","df = pd.read_csv('your_data.csv')\n","\n","# Preprocess the text\n","processed_df, metrics_overall, metrics_instance = preprocess_df(df)\n","```"]},{"cell_type":"markdown","metadata":{"id":"N5XtyaP6NsDH"},"source":["## 📊 Project Status\n","\n","### ✅ Completed\n","| Phase | Component | Status |\n","|-------|-----------|---------|\n","| Data Preparation | Dataset Creation | ✓ Complete |\n","| Data Preparation | Dataset Expansion | ✓ Complete |\n","| Data Preparation | Preprocessing Pipeline | ✓ Complete |\n","\n","### 🔄 In Progress\n","| Phase | Component | Status |\n","|-------|-----------|---------|\n","| Model Development | Feature Extraction | 🚧 In Progress |\n","| Model Development | Dataset Splitting | 📋 Planned |\n","| Model Development | Model Selection | 📋 Planned |\n","\n","### ⏳ Upcoming\n","| Phase | Component | Status |\n","|-------|-----------|---------|\n","| Evaluation | Model Evaluation | ⏳ Planned |\n","| Evaluation | Error Analysis | ⏳ Planned |\n","| Deployment | Model Packaging | ⏳ Planned |\n","\n","See [STEPS.md](STEPS.md) for detailed progress tracking."]},{"cell_type":"markdown","metadata":{"id":"MQM9K0G5NsDI"},"source":["## 📚 Resources\n","\n","### Dataset\n","\n","The project uses the [Netflix & Facebook Posts Dataset](https://www.kaggle.com/datasets/tomthescientist/netflix-facebook-posts-as-sentences-for-llm-input) from Kaggle:\n","\n","- **Provider:** Tom the Scientist\n","- **Platform:** Kaggle\n","- **Content:** Collection of Netflix and Facebook posts\n","- **Purpose:** Training data for fragment detection\n","\n","### External Links\n","\n","- 📊 [Dataset Link](https://www.kaggle.com/datasets/tomthescientist/netflix-facebook-posts-as-sentences-for-llm-input)\n","- 📓 [Project Documentation](https://github.com/alaamer12/Fragma)\n","- 🧪 [Colab Notebooks drive](https://drive.google.com/drive/folders/14elEhg-Kb9UXUtaUIvJrEbpFroEq_mUB?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"qMx7jFgCNsDI"},"source":["## 👥 Contributors\n","\n","| 👤 Name | 🧑‍💻 Role | 📬 GitHub | 🔗 LinkedIn |\n","|---------|----------|-----------|------------|\n","| Amr Muhamed | Maintainer | [alaamer12](https://github.com/alaamer12) | [alaamer12](www.linkedin.com/in/amr-muhamed-0b0709265) |\n","| Muhamed Ibrahim | Data Engineer | [muhammad-senna](https://github.com/muhammad-senna) | [muhammad-senna](https://linkedin.com/in/muhammad-senna) |\n","\n","© 2025 Amr Muhamed. All Rights Reserved.\n","\n","*Last updated: May 13, 2025*"]},{"cell_type":"markdown","metadata":{"id":"W8wGukjvNsDI"},"source":["## 📝 Documentation\n","\n","### Key Files:\n","\n","- [**README.md**](README.md): Project overview and setup instructions\n","- [**FD.md**](FD.md): Detailed fragment detection documentation\n","- [**STEPS.md**](STEPS.md): Project roadmap and progress tracking\n","\n","### 📋 Key Features\n","\n","- Intelligent sentence splitting based on linguistic patterns\n","- Comprehensive text preprocessing pipeline\n","- Pattern-based feature detection (adverbs, past tense, gerunds)\n","- Dataset balancing strategies (reduction/expansion)\n","- Detailed preprocessing metrics and tracking\n","\n","---\n","\n","© 2025 Amr Muhamed. All Rights Reserved.\n","\n","*Last updated: May 12, 2025*"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}